{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsallrounder/NLP-Workshop-2022/blob/main/NLP_Workshop_NLP_Projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkgQL_3TbYao",
        "outputId": "cf03b1a7-4d3e-4326-dfd2-b028d5b44b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n",
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent summary:\n",
            "Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken.\n",
            "The development of NLP applications is challenging because computers traditionally require humans to 'speak' to them in a programming language that is precise, unambiguous and highly structured, or through a limited number of clearly enunciated voice commands.\n",
            "\n",
            "\n",
            "Word count summary:\n",
            "Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken.\n"
          ]
        }
      ],
      "source": [
        "# Simple Text Summarization in Python\n",
        "\n",
        "# import the gensim module and summarize function\n",
        "from gensim.summarization.summarizer import summarize \n",
        "\n",
        "# Paragraph\n",
        "paragraph = \"Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. NLP is a component of artificial intelligence (AI). The development of NLP applications is challenging because computers traditionally require humans to 'speak' to them in a programming language that is precise, unambiguous and highly structured, or through a limited number of clearly enunciated voice commands. Human speech, however, is not always precise -- it is often ambiguous and the linguistic structure can depend on many complex variables, including slang, regional dialects and social context.\"\n",
        "\n",
        "# Get the Summary of the text based on percentage (0.5% of the original content). \n",
        "summ_per = summarize(paragraph, ratio = 0.7) \n",
        "print(\"Percent summary:\") \n",
        "print(summ_per) \n",
        "\n",
        "# Get the summary of the text based on number of words (50 words) \n",
        "summ_words = summarize(paragraph, word_count = 30) \n",
        "print(\"\\n\")\n",
        "print(\"Word count summary:\") \n",
        "print(summ_words) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automated Keyword Extraction\n",
        "\n",
        "# import the gensim module and keywords function\n",
        "from gensim.summarization import keywords\n",
        "\n",
        "# Paragraph\n",
        "paragraph = \"Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. NLP is a component of artificial intelligence (AI). The development of NLP applications is challenging because computers traditionally require humans to 'speak' to them in a programming language that is precise, unambiguous and highly structured, or through a limited number of clearly enunciated voice commands. Human speech, however, is not always precise -- it is often ambiguous and the linguistic structure can depend on many complex variables, including slang, regional dialects and social context.\"\n",
        "\n",
        "# Get the keywords from the paragraph \n",
        "keywords_txt = keywords(paragraph) \n",
        "print(keywords_txt) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpaVOzyNbZq2",
        "outputId": "fbb4a950-7a31-4adc-cb3f-7a4d8757a3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human\n",
            "humans\n",
            "language\n",
            "nlp\n",
            "structured\n",
            "structure\n",
            "regional\n",
            "variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python code to measure similarity between two sentences using similarity. \n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sentences\n",
        "s1 = nlp(\"The weather is rainy.\")\n",
        "s2 = nlp(\"It is going to rain outside.\")\n",
        "\n",
        "# Calculate the similarity\n",
        "print(\"The similarity is:\",s1.similarity(s2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhl4vdCNcON7",
        "outputId": "f00bbd66-43ad-4fca-ef48-0114251a6840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity is: 0.38663976003010325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Correction"
      ],
      "metadata": {
        "id": "uS42e1qpj4GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "words = [\"Machne\", \"Learnin\"]\n",
        "corrected_words = []\n",
        "for i in words:\n",
        "    corrected_words.append(TextBlob(i))\n",
        "print(\"Wrong words :\", words)\n",
        "print(\"Corrected Words are :\")\n",
        "for i in corrected_words:\n",
        "    print(i.correct(), end=\" \")"
      ],
      "metadata": {
        "id": "WQ_veHFqj9fM",
        "outputId": "b4de0aee-a5d4-4ed2-8235-a13b71cf651f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong words : ['Machne', 'Learnin']\n",
            "Corrected Words are :\n",
            "Machine Learning "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "hp-JOYgwgdu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing NLTK and Downloading the Data\n",
        "\n",
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0QsXaricjPS",
        "outputId": "1ff9eae3-6d8f-4bf4-83ab-29f2257e3fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the Data\n",
        "\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "uzuLJ0_ndj1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "metadata": {
        "id": "CKCnZ9SNd3RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The punkt module is a pre-trained model that helps you tokenize words and sentences.\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_6Boszmd8qG",
        "outputId": "3d113776-23a0-42ec-f64f-b2f8d6517416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
        "\n",
        "print(tweet_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg8f4PYDeAdr",
        "outputId": "d42b5729-39b9-4e12-8061-1e4d34bb3c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wordnet is a lexical database for the English language that helps the script determine the base word.\n",
        "#  You need the averaged_perceptron_tagger resource to determine the context of a word in a sentence.\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNhs-FZ1eExa",
        "outputId": "b7de31dc-8c38-4621-821d-e770bddb48ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlVb4YMjeTc5",
        "outputId": "fe05b6c1-5ad2-45a2-93d1-675a22eb28ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence\n",
        "\n",
        "print(lemmatize_sentence(tweet_tokens[0]))\n",
        "\n",
        "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgnh9IXLeXYH",
        "outputId": "f63e3949-5215-4708-8b3e-1f94d3fac5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Noise from the Data\n",
        "\n",
        "import re, string\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "ttbDs2qYed7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(remove_noise(tweet_tokens[0], stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiXWeObQeyZF",
        "outputId": "61000f34-4088-4675-af57-4cb1ef0fb0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#print(remove_noise(tweet_tokens[0], stop_words))\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "TBF_TVZhe0y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "print(positive_tweet_tokens[500])\n",
        "print(positive_cleaned_tokens_list[500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXEPMdSgfEPY",
        "outputId": "b276c9d4-7617-4e31-d517-23ee2bac047a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "['dang', 'rad', '#fanart', ':d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "\n",
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "pZayNFqOfI-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAyoH_SwfTUo",
        "outputId": "6dfe387c-db90-47f0-c9e7-5338393903b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "q3htqSZXfXEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "3nsiiFl5fZ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1N1FJuHfbum",
        "outputId": "4bb3fa62-33c9-4322-9641-1c376c418f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9946666666666667\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2085.9 : 1.0\n",
            "                      :) = True           Positi : Negati =    979.3 : 1.0\n",
            "                     bam = True           Positi : Negati =     23.3 : 1.0\n",
            "                follower = True           Positi : Negati =     20.7 : 1.0\n",
            "                    glad = True           Positi : Negati =     20.0 : 1.0\n",
            "                     sad = True           Negati : Positi =     18.8 : 1.0\n",
            "                      aw = True           Negati : Positi =     17.2 : 1.0\n",
            "                     x15 = True           Negati : Positi =     15.9 : 1.0\n",
            "                followed = True           Negati : Positi =     14.8 : 1.0\n",
            "                 welcome = True           Positi : Negati =     13.3 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"The Foood Was Good\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7gsuR8LfdnW",
        "outputId": "7fb6a4ac-1fe4-4572-a51d-f0c1e486740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtQi60Hxfhvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Rule Bases ChatBot"
      ],
      "metadata": {
        "id": "aPkQ9_rOzSAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chat.util import Chat, reflections"
      ],
      "metadata": {
        "id": "XkWhUOVEzV3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pairs is a list of patterns and responses.\n",
        "pairs = [\n",
        "    [\n",
        "        r\"(.*)my name is (.*)\",\n",
        "        [\"Hello %2, How are you today ?\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*)help(.*) \",\n",
        "        [\"I can help you \",]\n",
        "    ],\n",
        "     [\n",
        "        r\"(.*) your name ?\",\n",
        "        [\"My name is Ahya, but you can just call me robot and I'm a chatbot .\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"how are you (.*) ?\",\n",
        "        [\"I'm doing very well\", \"i am great !\"]\n",
        "    ],\n",
        "    [\n",
        "        r\"sorry (.*)\",\n",
        "        [\"Its alright\",\"Its OK, never mind that\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"i'm (.*) (good|well|okay|ok)\",\n",
        "        [\"Nice to hear that\",\"Alright, great !\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"(hi|hey|hello|hola|holla)(.*)\",\n",
        "        [\"Hello\", \"Hey there\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"what (.*) want ?\",\n",
        "        [\"Make me an offer I can't refuse\",]\n",
        "        \n",
        "    ],\n",
        "    [\n",
        "        r\"(.*)created(.*)\",\n",
        "        [\"Created By NLTK created me using Python's NLTK library \",\"top secret ;)\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*) (location|city) ?\",\n",
        "        ['Banglore, India',]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*)raining in (.*)\",\n",
        "        [\"No rain in the past 4 days here in %2\",\"In %2 there is a 50% chance of rain\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"how (.*) health (.*)\",\n",
        "        [\"Health is very important, but I am a computer, so I don't need to worry about my health \",]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*)(sports|game|sport)(.*)\",\n",
        "        [\"I'm a very big fan of Cricket\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"who (.*) (Cricketer|Batsman)?\",\n",
        "        [\"Virat Kohli\"]\n",
        "    ],\n",
        "    [\n",
        "        r\"quit\",\n",
        "        [\"Bye for now. See you soon :) \",\"It was nice talking to you.\"]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*)\",\n",
        "        ['That is nice to hear']\n",
        "    ],\n",
        "]"
      ],
      "metadata": {
        "id": "lCarj-01zZNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reflections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyZMvvv70aIQ",
        "outputId": "408fa5d9-49e9-4215-9831-2219f272592a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i am': 'you are', 'i was': 'you were', 'i': 'you', \"i'm\": 'you are', \"i'd\": 'you would', \"i've\": 'you have', \"i'll\": 'you will', 'my': 'your', 'you are': 'I am', 'you were': 'I was', \"you've\": 'I have', \"you'll\": 'I will', 'your': 'my', 'yours': 'mine', 'you': 'me', 'me': 'you'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_dummy_reflections= {\n",
        "    \"go\"     : \"gone\",\n",
        "    \"hello\"    : \"hey there\"\n",
        "}"
      ],
      "metadata": {
        "id": "e_u0HYCY0eFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#default message at the start of chat\n",
        "print(\"Hi, I'm Ahya and I like to chat\\nPlease type lowercase English language to start a conversation. Type quit to leave \")\n",
        "#Create Chat Bot\n",
        "chat = Chat(pairs, reflections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TsWklly0gxI",
        "outputId": "1a705f1a-0220-4ddb-a951-350f0fb7bfec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, I'm Ahya and I like to chat\n",
            "Please type lowercase English language to start a conversation. Type quit to leave \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.converse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVWajzqU0quP",
        "outputId": "413953ab-cabe-43f2-e882-3e425db8446b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">quit\n",
            "Bye for now. See you soon :) \n"
          ]
        }
      ]
    }
  ]
}